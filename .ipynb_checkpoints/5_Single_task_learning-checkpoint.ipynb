{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from random import getrandbits\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from random import randint\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1. / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_floats(low, high, k=1):\n",
    "    \"\"\" Return a k-length list of unique random floats\n",
    "        in the range of low <= x <= high\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    seen = set()\n",
    "    for i in range(k):\n",
    "        x = random.uniform(low, high)\n",
    "        while x in seen:\n",
    "            x = random.uniform(low, high)\n",
    "        seen.add(x)\n",
    "        result.append(x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(pt):\n",
    "    df=pd.read_csv(str(pt) + '/4_time_line.csv', header=0, sep=',')\n",
    "    df['week']=df.loc[:,\"week_click\":\"week_undisp\"].min(axis=1)\n",
    "    col=[col for col in df.columns if col not in [\"week_click\",\"week_book\",\"week_reply\",\"week_undisp\"]]\n",
    "    df=df[col]\n",
    "    user_list=np.unique(df.user_id)\n",
    "    #split a provided dataset by taking into account the time_line \n",
    "    test=[]\n",
    "    train=[]\n",
    "    for i in tqdm_notebook(range(len(user_list))):\n",
    "        df_split = np.array_split(df[df.user_id==user_list[i]].sort_values(by='week'), 3)\n",
    "        train.append(list(df_split[0].index))\n",
    "        train.append(list(df_split[1].index))\n",
    "        test.append(list(df_split[2].index))\n",
    "    train=list(itertools.chain(*train))\n",
    "    test=list(itertools.chain(*test))\n",
    "#======================================\n",
    "    X_train=df[df.index.isin(train)]\n",
    "    X_test=df[df.index.isin(test)]\n",
    "    X_train.set_index(['user_id','item_id'], inplace=True)\n",
    "    X_test.set_index(['user_id','item_id'], inplace=True)\n",
    "#======================================\n",
    "    Y_train_click=X_train.click\n",
    "    Y_train_book=X_train.book\n",
    "    Y_train_reply=X_train.reply\n",
    "    \n",
    "    Y_test_click=X_test.click\n",
    "    Y_test_book=X_test.book\n",
    "    Y_test_reply=X_test.reply\n",
    "#======================================    \n",
    "    col=[col for col in df.columns if col not in ['displayed','user_id','item_id',\"click\",\"book\",\"reply\",\"week\"]]\n",
    "    X_train=X_train[col]\n",
    "    X_test=X_test[col]\n",
    "    \n",
    "    X_train = X_train.astype(np.float64)\n",
    "    X_test = X_test.astype(np.float64)\n",
    "    #min_max normalization just for contextual features\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X_train_minmax = min_max_scaler.fit_transform(X_train.iloc[:,15:])\n",
    "    X_test_minmax = min_max_scaler.transform(X_test.iloc[:,15:])\n",
    "\n",
    "    X_train_minmax = pd.DataFrame(X_train_minmax)\n",
    "    X_test_minmax = pd.DataFrame(X_test_minmax)\n",
    "    X_train_minmax.columns = X_train.iloc[:,15:].columns\n",
    "    X_test_minmax.columns = X_test.iloc[:,15:].columns\n",
    "    X_train_minmax.index = X_train.iloc[:,15:].index\n",
    "    X_test_minmax.index = X_test.iloc[:,15:].index\n",
    "    #standardization just for contextual features\n",
    "    sc=StandardScaler()\n",
    "    sc.fit(X_train_minmax)\n",
    "    X_train_std=sc.transform(X_train_minmax)\n",
    "    X_test_std=sc.transform(X_test_minmax)\n",
    "\n",
    "    X_train_std = pd.DataFrame(X_train_std)\n",
    "    X_test_std = pd.DataFrame(X_test_std)\n",
    "    X_train_std.columns = X_train.iloc[:,15:].columns\n",
    "    X_test_std.columns = X_test.iloc[:,15:].columns\n",
    "    X_train_std.index = X_train.iloc[:,15:].index\n",
    "    X_test_std.index = X_test.iloc[:,15:].index\n",
    "#=======================================\n",
    "    U=np.unique(X_train_std.reset_index().user_id)\n",
    "    I=np.unique(X_train_std.reset_index().item_id)\n",
    "    Y_train_click=Y_train_click.reset_index()\n",
    "    Y_train_book=Y_train_book.reset_index()\n",
    "    Y_train_reply=Y_train_reply.reset_index()\n",
    "    X_train_std=X_train_std.reset_index()\n",
    "    X_test_std=X_test_std.reset_index()\n",
    "    Y_test_click=Y_test_click.reset_index()\n",
    "    Y_test_book=Y_test_book.reset_index()\n",
    "    Y_test_reply=Y_test_reply.reset_index()\n",
    "#=========================Convert training and test sets from pandas dataframe to dictionaries\n",
    "    X_train_dict={}\n",
    "    for i in tqdm_notebook(xrange(len(X_train_std))):\n",
    "        X_train_dict[X_train_std.user_id[i],X_train_std.item_id[i]]=X_train.iloc[i,:15].values.tolist()+\\\n",
    "        X_train_std.iloc[i,2:].values.tolist()\n",
    "    X_test_dict={}\n",
    "    for i in tqdm_notebook(xrange(len(X_test_std))):\n",
    "            X_test_dict[X_test_std.user_id[i],X_test_std.item_id[i]]=[1]+X_test.iloc[i,:15].values.tolist()+\\\n",
    "            X_test_std.iloc[i,2:].values.tolist()\n",
    "\n",
    "   \n",
    "    Y_train_dict_click={}\n",
    "    Y_train_dict_book={}\n",
    "    Y_train_dict_reply={}\n",
    "    for i in tqdm_notebook(xrange(len(Y_train_click))):\n",
    "        Y_train_dict_click[Y_train_click.user_id[i],Y_train_click.item_id[i]]=Y_train_click.iloc[i,2]\n",
    "        Y_train_dict_book[Y_train_book.user_id[i],Y_train_book.item_id[i]]=Y_train_book.iloc[i,2]\n",
    "        Y_train_dict_reply[Y_train_reply.user_id[i],Y_train_reply.item_id[i]]=Y_train_reply.iloc[i,2]\n",
    "#======Create assistance dictionary where for eash user were assgined all offers  which were interacted by target user   \n",
    "    indx_dict={}\n",
    "    for i in range(len(U)):\n",
    "        indx_dict[U[i]]=X_train_std[X_train_std.user_id==U[i]].iloc[:,:2].values.tolist()\n",
    "#========In Stochastic Gradient Descent which we will use in further, the loss function defined as logistic loss function for Y\\in{-1,+1}   \n",
    "   \n",
    "    for key,values in Y_train_dict_click.iteritems():\n",
    "        if values == 0:\n",
    "            Y_train_dict_click[key] = -1\n",
    "    for key,values in Y_train_dict_book.iteritems():\n",
    "        if values == 0:\n",
    "            Y_train_dict_book[key] = -1\n",
    "    for key,values in Y_train_dict_reply.iteritems():\n",
    "        if values == 0:\n",
    "            Y_train_dict_reply[key] = -1\n",
    "            \n",
    "#=============Embedding representation for users and items\n",
    "\n",
    "    U_repr = {U[i]:[v for v in sample_floats(0,1,k=X_train.shape[1]-1)] for i in range (len(U))}\n",
    "    I_repr= {I[j]:[w for w in sample_floats(0,1,k=X_train.shape[1]-1)] for j in range (len(I))}\n",
    "\n",
    "#============Single-task learning for clicks task\n",
    "    \n",
    "    #Inputs:\n",
    "    eta=1e-6\n",
    "    lamb=1e-2\n",
    "    nb_runs = 600000\n",
    "    eps=1e-3\n",
    "    #Initialization:\n",
    "    w1=[w for w in sample_floats(0,1,k=32)]\n",
    "    global_loss = [1,0]\n",
    "    nb=1\n",
    "    while abs(global_loss[nb-1]-global_loss[nb])>eps:\n",
    "        loss_local=0\n",
    "        for j in tqdm_notebook(range(nb_runs)):\n",
    "            u = randint(0,len(U)-1)\n",
    "\n",
    "            target_samples=indx_dict[U[u]]\n",
    "            target_lables=[Y_train_dict_click[tuple(elem)] for elem in target_samples]\n",
    "            positive=[target_samples[m] for m in [l for l, e in enumerate(target_lables) if e == 1]]\n",
    "            negative=[target_samples[m] for m in [l for l, e in enumerate(target_lables) if e == -1]]\n",
    "\n",
    "            if (len(positive)!=0 and len(negative)!=0): \n",
    "\n",
    "                p=randint(0, len(positive)-1)\n",
    "                n=randint(0, len(negative)-1)\n",
    "\n",
    "\n",
    "                item_plus = X_train_dict[tuple(positive[p])]\n",
    "                item_minus = X_train_dict[tuple(negative[n])] \n",
    "\n",
    "                diff = (  np.array(item_minus) - np.array(item_plus)  ).tolist()\n",
    "                a = not getrandbits(1)\n",
    "                if a==True:\n",
    "                    l_1 = np.log( 1. + np.exp( np.dot([1.]+diff , w1 ) ) )\n",
    "                    dl_1=( [1.] + diff ) / (1. + np.exp( (-1.) * np.dot( [1.] + diff ,w1 ) ) )\n",
    "                else:\n",
    "                    l_1 = np.log(1.+ np.exp( np.dot( [-1.] + diff , w1 ) ) )\n",
    "                    dl_1=( [-1.] + diff ) / (1. + np.exp( (-1.) * np.dot( [-1.] + diff ,w1) ) )\n",
    "\n",
    "                w1 = w1 - eta * ( (dl_1) + (2 * lamb * np.sum( w1 ) ) )\n",
    "\n",
    "                loss_local = loss_local + l_1 + ( lamb * np.linalg.norm( w1,2 )**2 )\n",
    "\n",
    "\n",
    "        global_loss.append(loss_local/nb_runs)\n",
    "        nb+=1   \n",
    "        \n",
    "#============Single-task learning for bookmarks task\n",
    "\n",
    "    #Inputs:\n",
    "    eta=1e-6\n",
    "    lamb=1e-1\n",
    "    eps=1e-3\n",
    "    nb_runs = 600000\n",
    "    #Initialization:\n",
    "    w2=[w for w in sample_floats(0,1,k=32)]\n",
    "    global_loss=[0,1]\n",
    "    nb=1\n",
    "    while abs(global_loss[nb-1]-global_loss[nb])>eps:\n",
    "        loss_local=0\n",
    "        for j in tqdm_notebook(range(nb_runs)):\n",
    "            u = randint(0,len(U)-1)\n",
    "\n",
    "            target_samples=indx_dict[U[u]]\n",
    "            target_lables=[Y_train_dict_click[tuple(elem)] for elem in target_samples]\n",
    "            positive=[target_samples[m] for m in [l for l, e in enumerate(target_lables) if e == 1]]\n",
    "            negative=[target_samples[m] for m in [l for l, e in enumerate(target_lables) if e == -1]]\n",
    "\n",
    "            if (len(positive)!=0 and len(negative)!=0): \n",
    "\n",
    "                p=randint(0, len(positive)-1)\n",
    "                n=randint(0, len(negative)-1)\n",
    "\n",
    "                book_plus_label = Y_train_dict_book[tuple(positive[p])]\n",
    "                book_minus_label = Y_train_dict_book[tuple(negative[n])]\n",
    "                reply_plus_label = Y_train_dict_reply[tuple(positive[p])]\n",
    "                reply_minus_label = Y_train_dict_reply[tuple(negative[n])]\n",
    "\n",
    "                item_plus = X_train_dict[tuple(positive[p])]\n",
    "                item_minus = X_train_dict[tuple(negative[n])] \n",
    "\n",
    "                diff = ( np.array(item_minus) - np.array(item_plus) ).tolist()            \n",
    "\n",
    "                l_2_plus = (1./2) * np.log( 1 + ( 1. / np.exp( book_plus_label * np.dot([1.] + item_plus , w2 )  ) ) ) \n",
    "                l_2_minus = (1./2) * np.log( 1 + ( 1. / np.exp( book_minus_label * np.dot([1.] + item_minus , w2 ) ) ) ) \n",
    "\n",
    "                dl_2_plus = ( 1./2) * ( ( (-1.)* book_plus_label * np.array([1.] + item_plus) ) / ( 1. + np.exp( book_plus_label * np.dot( [1.] + item_plus , w2 ) ) ) ) \n",
    "                dl_2_minus = ( 1./2) * ( ( (-1.)* book_minus_label * np.array([1.] + item_minus) ) / ( 1. + np.exp( book_minus_label * np.dot( [1.] + item_minus , w2 ) ) ) ) \n",
    "\n",
    "                if book_plus_label == 1 :\n",
    "                    dl_2_plus = 7 * dl_2_plus\n",
    "                if book_minus_label == 1 :\n",
    "                    dl_2_minus = 7 * dl_2_minus\n",
    "\n",
    "\n",
    "                w2 = w2 - eta * ( (dl_2_plus + dl_2_minus) + (2 * lamb * np.sum( w2 ) ) )\n",
    "\n",
    "                loss_local = loss_local + l_2_plus + l_2_minus + ( lamb * np.linalg.norm( w2,2 )**2 )\n",
    "\n",
    "\n",
    "\n",
    "        global_loss.append(loss_local/nb_runs)\n",
    "        nb+=1\n",
    "\n",
    "#============Single-task learning for replied task\n",
    "    #Inputs:\n",
    "    eta=1e-6\n",
    "    lamb=1e-1\n",
    "    eps=1e-3\n",
    "    nb_runs = 600000\n",
    "    #Initialization:\n",
    "    w3=[w for w in sample_floats(0,1,k=32)]\n",
    "    global_loss=[0,1]\n",
    "    nb=1\n",
    "    while abs(global_loss[nb-1]-global_loss[nb])>eps:\n",
    "        loss_local=0\n",
    "        for j in tqdm_notebook(range(nb_runs)):\n",
    "            u = randint(0,len(U)-1)\n",
    "\n",
    "            target_samples=indx_dict[U[u]]\n",
    "            target_lables=[Y_train_dict_click[tuple(elem)] for elem in target_samples]\n",
    "            positive=[target_samples[m] for m in [l for l, e in enumerate(target_lables) if e == 1]]\n",
    "            negative=[target_samples[m] for m in [l for l, e in enumerate(target_lables) if e == -1]]\n",
    "\n",
    "            if (len(positive)!=0 and len(negative)!=0): \n",
    "\n",
    "                p=randint(0, len(positive)-1)\n",
    "                n=randint(0, len(negative)-1)\n",
    "\n",
    "                book_plus_label = Y_train_dict_book[tuple(positive[p])]\n",
    "                book_minus_label = Y_train_dict_book[tuple(negative[n])]\n",
    "                reply_plus_label = Y_train_dict_reply[tuple(positive[p])]\n",
    "                reply_minus_label = Y_train_dict_reply[tuple(negative[n])]\n",
    "\n",
    "                item_plus = X_train_dict[tuple(positive[p])]\n",
    "                item_minus = X_train_dict[tuple(negative[n])] \n",
    "\n",
    "                diff = ( np.array(item_minus) - np.array(item_plus) ).tolist()            \n",
    "\n",
    "                l_3_plus = (1./2) * np.log( 1 + ( 1. / np.exp( reply_plus_label * np.dot( [1.] + item_plus , w3 )  ) ) ) \n",
    "                l_3_minus = (1./2) * np.log( 1 + ( 1. / np.exp( reply_minus_label * np.dot( [1.] + item_minus , w3 ) ) ) )\n",
    "\n",
    "                dl_3_plus = ( 1./2) * ( ( (-1.)* reply_plus_label * np.array([1.] + item_plus) ) / ( 1. + np.exp( reply_plus_label * np.dot( [1.] + item_plus , w3 ) ) ) ) \n",
    "                dl_3_minus = ( 1./2) * ( ( (-1.)* reply_minus_label * np.array([1.] + item_minus) ) / ( 1. + np.exp( reply_minus_label * np.dot( [1.] + item_minus , w3 ) ) ) )\n",
    "\n",
    "                if reply_plus_label == 1 :\n",
    "                    dl_3_plus = 5 * dl_3_plus\n",
    "                if reply_minus_label == 1 :\n",
    "                    dl_3_minus = 5 * dl_3_minus\n",
    "\n",
    "\n",
    "                w3 = w3 - eta * ( (dl_3_plus + dl_3_minus) + (2 * lamb * np.sum( w3 ) ) )    \n",
    "\n",
    "                loss_local = loss_local + l_3_plus + l_3_minus  + ( lamb * np.linalg.norm( w3,2 )**2 )\n",
    "\n",
    "\n",
    "\n",
    "        global_loss.append(loss_local/nb_runs)\n",
    "        nb+=1\n",
    "#===============================================================\n",
    "    Y_pred_click={}\n",
    "    Y_pred_book={}\n",
    "    Y_pred_reply={}\n",
    "    for key,value in X_test_dict.iteritems():\n",
    "        Y_pred_click[key]=sigmoid(np.dot(w1,value))\n",
    "    for key,value in X_test_dict.iteritems():\n",
    "        Y_pred_book[key]=sigmoid(np.dot(w2,value))\n",
    "    for key,value in X_test_dict.iteritems():\n",
    "        Y_pred_reply[key]=sigmoid(np.dot(w3,value))\n",
    "        \n",
    "    for key,value in Y_pred_click.iteritems():\n",
    "        if value<=0.5:\n",
    "            Y_pred_click[key]=0\n",
    "        else:\n",
    "            Y_pred_click[key]=1\n",
    "    for key,value in Y_pred_book.iteritems():\n",
    "        if value<=0.5:\n",
    "            Y_pred_book[key]=0\n",
    "        else:\n",
    "            Y_pred_book[key]=1\n",
    "    for key,value in Y_pred_reply.iteritems():\n",
    "        if value<=0.5:\n",
    "            Y_pred_reply[key]=0\n",
    "        else:\n",
    "            Y_pred_reply[key]=1\n",
    "    \n",
    "    Y_pred_click_pd=pd.DataFrame(Y_pred_click.items())\n",
    "    Y_pred_book_pd=pd.DataFrame(Y_pred_book.items())\n",
    "    Y_pred_reply_pd=pd.DataFrame(Y_pred_reply.items())\n",
    "    \n",
    "    Y_pred_click_pd[['user_id','item_id']]=pd.DataFrame(Y_pred_click_pd[0].values.tolist())\n",
    "    Y_pred_book_pd[['user_id','item_id']]=pd.DataFrame(Y_pred_book_pd[0].values.tolist())\n",
    "    Y_pred_reply_pd[['user_id','item_id']]=pd.DataFrame(Y_pred_reply_pd[0].values.tolist())\n",
    "    Y_pred_click_pd = Y_pred_click_pd[[\"user_id\",\"item_id\",1]]\n",
    "    Y_pred_book_pd = Y_pred_book_pd[[\"user_id\",\"item_id\",1]]\n",
    "    Y_pred_reply_pd = Y_pred_reply_pd[[\"user_id\",\"item_id\",1]]\n",
    "    FINAL_click=pd.merge(Y_test_click,Y_pred_click_pd,how='left',on=['user_id','item_id'])\n",
    "    FINAL_book=pd.merge(Y_test_book,Y_pred_book_pd,how='left',on=['user_id','item_id'])\n",
    "    FINAL_reply=pd.merge(Y_test_reply,Y_pred_reply_pd,how='left',on=['user_id','item_id'])\n",
    "    \n",
    "    print('click')\n",
    "    print(classification_report(FINAL_click.click, FINAL_click[1]))\n",
    "    print(confusion_matrix(FINAL_click.click, FINAL_click[1]))\n",
    "    print('book')\n",
    "    print(classification_report(FINAL_book.book, FINAL_book[1]))\n",
    "    print(confusion_matrix(FINAL_book.book, FINAL_book[1]))\n",
    "    print('reply')\n",
    "    print(classification_report(FINAL_reply.reply, FINAL_reply[1]))\n",
    "    print(confusion_matrix(FINAL_reply.reply, FINAL_reply[1]))\n",
    "    \n",
    "    false_positive_rate, recall, thresholds = roc_curve(FINAL_click.click, FINAL_click[1])\n",
    "    roc_auc = auc(false_positive_rate, recall)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(false_positive_rate, recall, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Fall-out')\n",
    "    plt.show()\n",
    "    \n",
    "    false_positive_rate, recall, thresholds = roc_curve(FINAL_book.book, FINAL_book[1])\n",
    "    roc_auc = auc(false_positive_rate, recall)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(false_positive_rate, recall, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Fall-out')\n",
    "    plt.show()\n",
    "    \n",
    "    false_positive_rate, recall, thresholds = roc_curve(FINAL_reply.reply, FINAL_reply[1])\n",
    "    roc_auc = auc(false_positive_rate, recall)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(false_positive_rate, recall, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Fall-out')\n",
    "    plt.show()\n",
    "    \n",
    "    scores={}\n",
    "    for key,value in X_test_dict.iteritems():\n",
    "        scores[key]=np.dot(value,w1)\n",
    "\n",
    "    Y_test_click=Y_test_click.set_index(['user_id','item_id'])\n",
    "    Y_test_dict=Y_test_click.to_dict('index')\n",
    "    for key,value in scores.iteritems():\n",
    "        scores[key]=[value]+[Y_test_dict[key]['click']]\n",
    "    Scores=pd.DataFrame(scores.items())\n",
    "    Scores[['user_id','item_id']]=pd.DataFrame(Scores[0].values.tolist())\n",
    "    Scores[['score','Y_test_click']]=pd.DataFrame(Scores[1].values.tolist())\n",
    "    user_list=np.unique(X_test_std.user_id)\n",
    "    total_map=[]\n",
    "    k=1\n",
    "    for i in tqdm_notebook(range (len(user_list))):\n",
    "        target_list=Scores[Scores.user_id==user_list[i]].sort_values(by='score',ascending=False)\n",
    "        up=1\n",
    "        down=1\n",
    "        precision=[]\n",
    "        for j,score in enumerate(target_list.Y_test_click[:k].values.tolist()):\n",
    "            if score!=0:\n",
    "                precision.append(float(up)/float(down+j))\n",
    "                up+=1\n",
    "            else:\n",
    "                precision.append(0)\n",
    "        total_map.append(np.sum(precision[0:k])/k)\n",
    "    np.mean(total_map)\n",
    "\n",
    "\n",
    "       \n",
    "    return w1,w2,w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(\"/Users/amirasarbaev/Desktop\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
