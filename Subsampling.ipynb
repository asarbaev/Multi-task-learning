{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were providen 4 different .csv files. \n",
    "In this chapter we are going to subsample interactions.csv file by user activity and offer popularity. \n",
    "Thus, we set threshold which is equal 30.\n",
    "i.e. we are going to remove those users who have a number of interactions with unique offers less than 30 times\n",
    "and the same for offers popularity: we are going to remove offers which were interacted by unique users less than 30 times.\n",
    "After, we are going to take impressions.csv file where were provided information about displayed offers for users and remove all users and offer who are absent in interaction dataset after subsampling.\n",
    "Input: interactions.csv and impressions.csv\n",
    "Output: single dataset with following structure  -> user_id\titem_id\tinteraction_type\tdisplayed\tcreated_at. \n",
    "Where interaction_type can take values from 0 to 3 (0-non-interacted, 1-clicked, 2-bookmarked,3 replied)\n",
    "displyed can take values either 0 or 1 (0 - curent offer didn't show to current user, 1 - otherwise)\n",
    "created_at is the number of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import spatial\n",
    "import datetime\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "#p - path to the directory of the provided dataset\n",
    "#th - threshold for subsampling\n",
    "#s - path to the directory where you want to save the file\n",
    "\n",
    "def SS_dataset(p,th,s):\n",
    "    df = pd.read_csv( str ( p ) + '/interactions.csv', header = 0 , sep = '\\t' )\n",
    "    df = df.drop( df [ df.interaction_type == 4 ].index )                                             #remove \"delete\" type of interactions\n",
    "    df['created_at'] = pd.to_datetime( df['created_at'], unit = 's' )\n",
    "    data = df.groupby( ['user_id','item_id'] )['interaction_type'].count().reset_index()           #calculate number of unique pairs(user,item)\n",
    "    data.interaction_type = np.where( data.interaction_type > 1 , 1 , data.interaction_type ) \n",
    "    data = data.groupby( ['item_id'],sort=True ).sum()\n",
    "    reject_list = data[(data.interaction_type < th)]\n",
    "    reject_items = list(reject_list.index)\n",
    "    df = df[~df['item_id'].isin(reject_items)]\n",
    "    data = df.groupby(['user_id','item_id'])['interaction_type'].count().reset_index()\n",
    "    data.interaction_type = np.where( data.interaction_type > 1 , 1 , data.interaction_type )\n",
    "    data = data.groupby(['user_id'],sort=True).sum()\n",
    "    reject_list = data[(data.interaction_type < th)]\n",
    "    reject_users = list(reject_list.index)\n",
    "    df = df[~df['user_id'].isin(reject_users)]\n",
    "    \n",
    "    f = open(str(p)+'/impressions.csv', 'rb')\n",
    "    reader = csv.reader(f,delimiter='\\t')\n",
    "    headers = reader.next()\n",
    "    column = {}\n",
    "    for h in headers:\n",
    "        column[h] = []\n",
    "\n",
    "    for row in reader:\n",
    "        for h, v in zip(headers, row):\n",
    "            column[h].append(v)\n",
    "    for i in range (len(column['items'])):\n",
    "        column['items'][i]=map(int, column['items'][i].split(\",\"))\n",
    "    column['user_id']=map(int, column['user_id'])\n",
    "    list_1=[]\n",
    "    for i in range (len(column['user_id'])):\n",
    "        us_list=[column['user_id'][i]]*len(set(column['items'][i]))\n",
    "        it_list=set(column['items'][i])\n",
    "        time_list=[column['week'][i]]*len(set(column['items'][i]))\n",
    "        list_1.append(zip(us_list,it_list,time_list))\n",
    "    list_1=list(set(list(itertools.chain.from_iterable(list_1))))                                               #took unique pairs\n",
    "    list_1=[list(tup)+[0]+[1] for tup in list_1]                                                                #convert to list of lists\n",
    "    df_list_1 = pd.DataFrame(list_1, columns=['user_id','item_id','created_at','interaction_type','displayed']) #df_list_1 is a list of all displyaed pairs from \"impressions\" file\n",
    "    df_list_1=df_list_1[['user_id','item_id','interaction_type','displayed','created_at']]\n",
    "    df_list_1 = df_list_1[df_list_1['item_id'].isin(df['item_id'])]\n",
    "    df_list_1 = df_list_1[df_list_1['user_id'].isin(df['user_id'])]\n",
    "    df.created_at=pd.to_datetime(df.created_at).dt.week\n",
    "    df_list_1g=df_list_1.groupby(['user_id','item_id']).count().reset_index()\n",
    "    \n",
    "    #merge interacted pairs with displayed to define interacted pairs that were not displayed\n",
    "    df = pd.merge( df, df_list_1g, left_on=['user_id','item_id'],right_on=['user_id','item_id'], how='left', indicator='flag')    \n",
    "    #df_list_undisp is list of all interacted pairs that weren't displayed\n",
    "    df_list_undisp=df[df['flag']=='left_only']\n",
    "    df_list_undisp['displayed'].fillna(0, inplace=True)\n",
    "    df_list_undisp=df_list_undisp[['user_id','item_id','interaction_type_x','displayed','created_at_x']]\n",
    "    df_list_undisp.columns=['user_id','item_id','interaction_type','displayed','created_at']\n",
    "    df_list_undisp['displayed']=df_list_undisp['displayed'].astype(np.int64)\n",
    "    \n",
    "    #df is a list of all interacted pairs that were displayed\n",
    "    df = df[df['flag']==\"both\"]\n",
    "    df = df[['user_id','item_id','interaction_type_x','displayed','created_at_x']]\n",
    "    df.columns = ['user_id','item_id','interaction_type','displayed','created_at']\n",
    "    df['displayed'] = df['displayed'].astype(np.int64)\n",
    "    \n",
    "    #df_list_2g is df_list_2 list that was grouped by user_id,item_id to define which pairs we should drop from impression file\n",
    "    df_list_2g=df.groupby(['user_id','item_id'],as_index=False).count()\n",
    "    \n",
    "    #merge df_list_1, df_list2g to define pairs should be droped from impression file\n",
    "    df_list_1=pd.merge(df_list_1, df_list_2g, on=['user_id','item_id'], how='left', indicator='flag')\n",
    "    #all displayed pairs from \"impressions\" file except of interacted pairs from \"interactions file\"\n",
    "    df_list_1=df_list_1[df_list_1['interaction_type_y'].isnull()]\n",
    "    df_list_1=df_list_1[['user_id','item_id','interaction_type_x','displayed_x','created_at_x']]\n",
    "    df_list_1.columns=['user_id','item_id','interaction_type','displayed','created_at']\n",
    "    \n",
    "    target_list=df_list_1.append([df,df_list_undisp])\n",
    "    target_list['displayed']=target_list['displayed'].astype(np.int64)\n",
    "    #combination of 3 lists. Where:\n",
    "    #1) df - list of all interacted pairs except of non-displayed\n",
    "    #2) df_list_undisp - list of all non-displayed interacted pairs\n",
    "    #3) df_list_1 - list of all displayed pairs except of pairs which are included in first two lists.\n",
    "    target_list.to_csv( str(s)+\"/1_target_list.csv\",index=None)\n",
    "    return target_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SS_dataset('/Users/amirasarbaev/Downloads/Internship/data',30,\"/Users/amirasarbaev/Desktop\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
